{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./cfg/yolo.cfg\n",
      "Parsing cfg/yolo.cfg\n",
      "Loading yolo/bin/yolo.weights ...\n",
      "Successfully identified 203934260 bytes\n",
      "Finished in 0.1176450252532959s\n",
      "Model has a coco model name, loading coco labels.\n",
      "\n",
      "Building net ...\n",
      "Source | Train? | Layer description                | Output size\n",
      "-------+--------+----------------------------------+---------------\n",
      "       |        | input                            | (?, 608, 608, 3)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 608, 608, 32)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 304, 304, 32)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 304, 304, 64)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | concat [16]                      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 64)\n",
      " Load  |  Yep!  | local flatten 2x2                | (?, 19, 19, 256)\n",
      " Load  |  Yep!  | concat [27, 24]                  | (?, 19, 19, 1280)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1    linear           | (?, 19, 19, 425)\n",
      "-------+--------+----------------------------------+---------------\n",
      "Running entirely on CPU\n",
      "Finished in 10.221611499786377s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import serial\n",
    "import threading\n",
    "import numpy as np\n",
    "from numpy import fft\n",
    "from numpy import array\n",
    "from sklearn import svm\n",
    "from pickle import BINSTRING\n",
    "from sklearn import grid_search\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "#Object dectection Yolo CNN\n",
    "import cv2\n",
    "sys.path.insert(0, '/home/rahulsingh/Desktop/earble/yolo')\n",
    "from darkflow.net.build import TFNet\n",
    "\n",
    "##########################################################################################################################\n",
    "#Step by step debugging in iPython: https://stackoverflow.com/questions/16867347/step-by-step-debugging-with-ipython\n",
    "#Multi-threading in python: https://stackoverflow.com/questions/31768865/how-to-use-threading-to-get-user-input-realtime-while-main-still-running-in-pyth\n",
    "##########################################################################################################################\n",
    "\n",
    "options = {\"model\": \"cfg/yolo.cfg\", \"load\": \"yolo/bin/yolo.weights\", \"threshold\": 0.2}\n",
    "tfnet = TFNet(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(data, timestamps):\n",
    "    num_seconds = float(timestamps[-2] - timestamps[0])\n",
    "    samples_per_second = len(data) / num_seconds\n",
    "    num_samples = len(data)\n",
    "    oscilations_per_sample = [float(oscilations) / num_samples for oscilations in range(0, num_samples)]\n",
    "    return [ops * samples_per_second for ops in oscilations_per_sample]\n",
    "\n",
    "def get_buckets(data, first, last, num_buckets, hertz_cutoff=float(5)):\n",
    "    # Transform all of the original data to be a single component along the first principal component\n",
    "    pca = PCA(n_components=1, copy=True, whiten=True)\n",
    "    transformed_dataset = PCA.fit_transform(pca, data)\n",
    "    #print(pca.explained_variance_ratio_)\n",
    "    slice=transformed_dataset[first:last]\n",
    "\n",
    "    transformed = fft.fft(slice)\n",
    "    absolute = [abs(complex) for complex in transformed]\n",
    "\n",
    "    frequencies = get_frequencies(data, (data.T)[0])\n",
    "\n",
    "    buckets = [0 for i in range(num_buckets)]\n",
    "    width = hertz_cutoff / num_buckets\n",
    "    sum_of_buckets = 0.0000001\n",
    "    for i in range(1, len(absolute)):\n",
    "        index = int(frequencies[i] / width)\n",
    "        if index >= num_buckets:\n",
    "            break;\n",
    "        buckets[index] += absolute[i]\n",
    "        sum_of_buckets += absolute[i]\n",
    "\n",
    "    if 0: #arguments['--normalize']:\n",
    "        buckets = map(lambda x: x/sum_of_buckets, buckets)\n",
    "\n",
    "    return buckets\n",
    "\n",
    "def get_samples(data, target):\n",
    "    result = []\n",
    "    intent = []\n",
    "    segmentsize=30\n",
    "    stride=5            # Reduce this to very little to get very large trainingsets\n",
    "    noOfBuckets=40\n",
    "    clas = np.array(target)\n",
    "    for  start in range(0, len(data) - segmentsize, stride):\n",
    "        if ((sum(clas[start: start + stride])<stride) & (sum(clas[start: start + stride])>0)): #((clas[start: start + stride]==).all()):\n",
    "            continue\n",
    "        if start + segmentsize <= len(data):\n",
    "            segments_buckets = get_buckets(data, start, start + segmentsize, noOfBuckets)\n",
    "            #print(segments_buckets)\n",
    "            intent.append(clas[start])\n",
    "            result.append(segments_buckets)\n",
    "    return result, intent\n",
    "\n",
    "def loadData(filename):\n",
    "    return np.loadtxt(filename, delimiter=\",\")\n",
    "\n",
    "def getPCASample(data, target):\n",
    "    result = []\n",
    "    intent = []\n",
    "    segmentsize=30\n",
    "    stride=2            # Reduce this to very little to get very large trainingsets\n",
    "    noOfBuckets=40\n",
    "    clas = np.array(target)\n",
    "    pca = PCA(n_components=1, copy=True, whiten=True)\n",
    "    \n",
    "    #Pre-processing the data\n",
    "    arr = np.array(data).T\n",
    "    new_arr = arr[1:13]\n",
    "    new_arr[0]  = (new_arr[0]*2)/ 32768.0\n",
    "    new_arr[1]  = (new_arr[1]*2)/ 32768.0\n",
    "    new_arr[2]  = (new_arr[2]*2)/ 32768.0\n",
    "    new_arr[3]  = (new_arr[3]*250)/ 32768.0\n",
    "    new_arr[4]  = (new_arr[4]*250)/ 32768.0\n",
    "    new_arr[5]  = (new_arr[5]*250)/ 32768.0\n",
    "    new_arr[6]  = (new_arr[6]*2)/ 32768.0\n",
    "    new_arr[7]  = (new_arr[7]*2)/ 32768.0\n",
    "    new_arr[8]  = (new_arr[8]*2)/ 32768.0\n",
    "    new_arr[9]  = (new_arr[9]*250)/ 32768.0\n",
    "    new_arr[10] = (new_arr[10]*250)/ 32768.0\n",
    "    new_arr[11] = (new_arr[11]*250)/ 32768.0\n",
    "    \n",
    "    new_arr = new_arr.T\n",
    "    \n",
    "    transformed_dataset = PCA.fit_transform(pca, new_arr)\n",
    "    #transformed_dataset = PCA.fit_transform(pca, data)\n",
    "    \n",
    "    for  start in range(0, len(data) - segmentsize, stride):\n",
    "        if ((sum(clas[start: start + stride])<stride) & (sum(clas[start: start + stride])>0)): #((clas[start: start + stride]==).all()):\n",
    "            continue\n",
    "        if start + segmentsize <= len(data):\n",
    "            intent.append(clas[start])\n",
    "            result.append(np.reshape(transformed_dataset[start:start+segmentsize], (30,)))\n",
    "    return result, intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a list/dictionary of known recognizable objects using yolo\n",
    "#Make a seperate counter for all of them\n",
    "#And threshold values for each\n",
    "#Convert counter into probability score\n",
    "\n",
    "#Open serial port for reading data from Intel Curie IMU\n",
    "ser0 = serial.Serial('/dev/ttyACM0', 115200) # Establish the connection on a specific port 0\n",
    "ser1 = serial.Serial('/dev/ttyACM1', 115200) # Establish the connection on a specific port 1\n",
    "ser0.flush()\n",
    "ser1.flush()\n",
    "\n",
    "#If want to train from imu data saved in file\n",
    "load = True\n",
    "if load:\n",
    "    clf = joblib.load('1s_6sps.pkl')\n",
    "    print(clf)\n",
    "else:\n",
    "    data, target = get_samples(np.array(imu), intent0)\n",
    "    svr = svm.SVC()\n",
    "    exponential_range = [pow(10, i) for i in range(-4, 1)]\n",
    "    parameters = {'kernel':['linear', 'rbf'], 'C':exponential_range, 'gamma':exponential_range}\n",
    "    clf = grid_search.GridSearchCV(svr, parameters, n_jobs=8, verbose=True)\n",
    "    clf.fit(data, target)\n",
    "    joblib.dump(clf, '1s_6sps.pkl')  #'../models/1s_6sps.pkl')\n",
    "    print(clf)\n",
    "\n",
    "camera = cv2.VideoCapture(1)\n",
    "result = list()\n",
    "counter= 0\n",
    "flag   = 0\n",
    "threshold  = 10\n",
    "intent_thres_imu = 10\n",
    "imu_data   = []\n",
    "pred_count = 0\n",
    "try:\n",
    "    while True:\n",
    "        ret, image = camera.read()\n",
    "        cv2.imshow('frame', image)\n",
    "        if(cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            break\n",
    "        out = (tfnet.return_predict(image))\n",
    "        #increment/decrement the counter ==> probably in logrithmic way\n",
    "        #then move to classifier SVM algorithm using IMU data\n",
    "        result.append(out)\n",
    "        for obj in out:\n",
    "            if((obj[\"label\"]==\"bottle\")==True):            #Check location of bottle as well to see if the bottle at particular location is detected\n",
    "                flag = 1\n",
    "                \n",
    "        if(flag):\n",
    "            counter = counter + 1\n",
    "            flag = 0\n",
    "            print(\"bottle counter: \", counter)\n",
    "        else:\n",
    "            if(counter>0):\n",
    "                counter = counter - 1\n",
    "                \n",
    "        if(counter > threshold):\n",
    "            print(\"Intention Probably is to pick up the bottle\")\n",
    "            #Do something with counter\n",
    "            while True: #as of now continous loop but should be more adaptive and intuitive\n",
    "                data_count = 0\n",
    "                ser0.flush()\n",
    "                ser1.flush()\n",
    "                while(data_count<30):       #read data from imu\n",
    "                    data0 = ser0.readline().rstrip()\n",
    "                    data0 = data0.split()\n",
    "                    data1 = ser1.readline().rstrip()\n",
    "                    data1 = data1.split()\n",
    "                    if (len(data0) == 6 & len(data1) == 6):\n",
    "                        imu_data.append([(int(data0[0])*2.0)/ 32768.0, (int(data0[1])*2.0)/ 32768.0, (int(data0[2])*2.0)/ 32768.0, (int(data0[3])*250.0)/32768.0, (int(data0[4])*250.0)/32768.0, (int(data0[5])*250.0)/32768.0])\n",
    "                        #imu_data.append([float(time.time()),  int(data0[0]), int(data0[1]), int(data0[2]), int(data0[3]), int(data0[4]), int(data0[5]), int(data1[0]), int(data1[1]), int(data1[2]), int(data1[3]), int(data1[4]), int(data1[5])])\n",
    "                        data_count = data_count + 1\n",
    "\n",
    "                pca = PCA(n_components=1, copy=True, whiten=True)\n",
    "                transformed_dataset = PCA.fit_transform(pca, np.array(imu_data[-30:]))\n",
    "                predicted = clf.predict(transformed_dataset.T)\n",
    "                \n",
    "                #bucket    = get_buckets(np.array(imu_data[-30:]), 1, 30, 40)   #send the last 30 elements data to get_samples function\n",
    "                #predicted = clf.predict(np.reshape(bucket, (1, 40)))           #reshaping and sending for classification\n",
    "                if(predicted == 1):\n",
    "                    print(\"IMU intent Prediction::: Positive Count\", pred_count)\n",
    "                    pred_count = pred_count + 1\n",
    "                    if(pred_count>intent_thres_imu):\n",
    "                        print(\"Intended Action is XYZ\")\n",
    "                else:\n",
    "                    print(\"IMU intent Prediction::: Negative Count\", pred_count)\n",
    "                    pred_count = pred_count - 1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ser0.close()\n",
    "    ser1.close()\n",
    "    pass\n",
    "    #del(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = 0\n",
    "#Using thread to store the user input for task classification\n",
    "def background():\n",
    "    global inp\n",
    "    while True:\n",
    "        inp = int(input())\n",
    "        \n",
    "#Now threading1 runs regardless of user input\n",
    "threading1 = threading.Thread(target=background)\n",
    "threading1.daemon = True\n",
    "threading1.start()\n",
    "\n",
    "ser0 = serial.Serial('/dev/ttyACM0', 115200) # Establish the connection on a specific port\n",
    "ser1 = serial.Serial('/dev/ttyACM1', 115200) # Establish the connection on a specific port\n",
    "counter0 = 0 \n",
    "counter1 = 0 \n",
    "fhandle  = open(\"imu.txt\", 'ab')\n",
    "fhandle0 = open(\"imu0.txt\", 'ab')\n",
    "fhandle1 = open(\"imu1.txt\", 'ab')\n",
    "ihandle0 = open(\"intent0.txt\", 'ab')\n",
    "ihandle1 = open(\"intent1.txt\", 'ab')\n",
    "imu0    = []  #contains tstamps with imu0 data\n",
    "imu1    = []  #contains tstamps with imu1 data\n",
    "imu     = []\n",
    "\n",
    "intent0 = []\n",
    "intent1 = []\n",
    "ser0.flush()\n",
    "ser1.flush()\n",
    "start0 = time.time()\n",
    "start1 = time.time()\n",
    "t0     = time.time()\n",
    "t1     = time.time()\n",
    "\n",
    "#FFT might have the User Activity information as well i.e. running, walking, sitting etc. which can also be used for counterchecks for intent classification\n",
    "#Or the above info can be captured as well in the SVM classifier\n",
    "while True:\n",
    "    try:\n",
    "        counter0 +=1\n",
    "        counter1 +=1\n",
    "        #ser.write(str(chr(counter))) # Convert the decimal number to ASCII then send it to the Arduino\n",
    "        data0 = ser0.readline().rstrip()\n",
    "        data0 = data0.split()\n",
    "        data1 = ser1.readline().rstrip()\n",
    "        data1 = data1.split()\n",
    "        if ((len(data0) == 6) & (len(data1) == 6) & (inp<2)):\n",
    "            imu0.append([float(time.time()), int(data0[0]), int(data0[1]), int(data0[2]), int(data0[3]), int(data0[4]), int(data0[5])])\n",
    "            imu1.append([float(time.time()), int(data1[0]), int(data1[1]), int(data1[2]), int(data1[3]), int(data1[4]), int(data1[5])])\n",
    "            imu.append([float(time.time()),  (int(data0[0])*2.0)/ 32768.0, (int(data0[1])*2.0)/ 32768.0, (int(data0[2])*2.0)/ 32768.0, (int(data0[3])*250.0)/32768.0, (int(data0[4])*250.0)/32768.0, (int(data0[5])*250.0)/32768.0, (int(data1[0])*2.0)/ 32768.0, (int(data1[1])*2.0)/ 32768.0, (int(data1[2])*2.0)/ 32768.0, (int(data1[3])*250.0)/32768.0, (int(data1[4])*250.0)/32768.0, (int(data1[5])*250.0)/32768.0])\n",
    "            intent0.append(inp)\n",
    "            intent1.append(inp)\n",
    "        if counter0 == 255:\n",
    "            print(\"frequency:\", 255/(time.time()-t0))\n",
    "            t0 = time.time()\n",
    "            counter0 = 0\n",
    "        if counter1 == 255:\n",
    "            print(\"frequency:\", 255/(time.time()-t1))\n",
    "            t1 = time.time()\n",
    "            counter1 = 0\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        end=time.time()\n",
    "        d_time0=end-start0\n",
    "        d_time1=end-start1\n",
    "        np.savetxt(fhandle0, np.array(imu0),delimiter=\",\")\n",
    "        fhandle0.close()\n",
    "        np.savetxt(fhandle1, np.array(imu1),delimiter=\",\")\n",
    "        fhandle1.close()\n",
    "        np.savetxt(ihandle0, np.array(intent0),delimiter=\",\")\n",
    "        ihandle0.close()\n",
    "        np.savetxt(ihandle1, np.array(intent1),delimiter=\",\")\n",
    "        ihandle1.close()\n",
    "        np.savetxt(fhandle, np.array(imu),delimiter=\",\")\n",
    "        fhandle.close()\n",
    "        print(\"Avg Frequency is :: \", len(imu0)/d_time0)\n",
    "        print(\"Avg Frequency is :: \", len(imu1)/d_time1)\n",
    "        ser0.close()\n",
    "        ser1.close()\n",
    "        pass\n",
    "        #exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 14160 points : 2005\n"
     ]
    }
   ],
   "source": [
    "###Naive Bayesian Approach\n",
    "#Choose distributions that best characterize your data and prediction problem\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "imuu = loadData(\"imu.txt\")\n",
    "inte= loadData(\"intent0.txt\")\n",
    "y_pred = gnb.fit(imuu, inte).predict(imuu)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (imuu.shape[0],(inte != y_pred).sum()))\n",
    "#Number of mislabeled points out of a total 150 points : 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14160,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:  1.1min\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-23-34906b94ac34>\", line 31, in <module>\n",
      "    clf.fit(X_train, Y_train)\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/sklearn/grid_search.py\", line 838, in fit\n",
      "    return self._fit(X, y, ParameterGrid(self.param_grid))\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/sklearn/grid_search.py\", line 574, in _fit\n",
      "    for parameters in parameter_iterable\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 789, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 699, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/multiprocessing/pool.py\", line 638, in get\n",
      "    self.wait(timeout)\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/multiprocessing/pool.py\", line 635, in wait\n",
      "    self._event.wait(timeout)\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/threading.py\", line 551, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/rahulsingh/anaconda3/envs/pyproj3/lib/python3.6/inspect.py\", line 71, in ismodule\n",
      "    return isinstance(object, types.ModuleType)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#ser0 = serial.Serial('/dev/ttyACM0', 115200) # Establish the connection on a specific port 0\n",
    "#ser1 = serial.Serial('/dev/ttyACM1', 115200) # Establish the connection on a specific port 1\n",
    "#ser0.flush()\n",
    "#ser1.flush()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "LOAD = False\n",
    "SHOW_CONFUSION_MATRIX = True\n",
    "\n",
    "intent_thres_imu = 10\n",
    "imu_data   = []\n",
    "pred_count = 0\n",
    "\n",
    "imuu = loadData(\"imu.txt\")\n",
    "inte= loadData(\"intent0.txt\")\n",
    "if LOAD:\n",
    "    clf = joblib.load('1s_6sps.pkl')\n",
    "    print(clf)\n",
    "else:\n",
    "    #data, target = get_samples(np.array(imu), intent0)\n",
    "    #data, target = getPCASample(imuu, inte)\n",
    "    data  = imuu\n",
    "    target= inte\n",
    "    svr   = svm.SVC()\n",
    "    exponential_range = [pow(10, i) for i in range(-4, 1)]\n",
    "    parameters = {'kernel':['linear', 'rbf'], 'C':exponential_range, 'gamma':exponential_range}\n",
    "    clf = grid_search.GridSearchCV(svr, parameters, n_jobs=8, verbose=True)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size=0.35, random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    joblib.dump(clf, '1s_6sps.pkl')  #'../models/1s_6sps.pkl')\n",
    "    print(clf)\n",
    "\n",
    "if SHOW_CONFUSION_MATRIX:\n",
    "    print(\"Confusion Matrix:\")\n",
    "    Y_predicted = clf.predict(X_test)\n",
    "    print(confusion_matrix(Y_test, Y_predicted))\n",
    "    \n",
    "    print(\"\\nBest estimator parameters: \")\n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "    #Calculates the score of the best estimator found.\n",
    "    score = clf.score(X_test, Y_test)\n",
    "    \n",
    "    print(\"\\nSCORE: {score}\\n\".format(score = score))\n",
    "    \n",
    "    print(\"Saving the model...\",)\n",
    "    \n",
    "    #Saves the model to the \"model.pkl\" file\n",
    "    joblib.dump(clf, 'model.pkl') \n",
    "    #Saves the classes to the \"classes.pkl\" file\n",
    "    #joblib.dump(classes, 'classes.pkl') \n",
    "    \n",
    "    print(\"DONE\")\n",
    "    \n",
    "\"\"\"\n",
    "print(\"Intention Probably is to pick up the bottle\")\n",
    "data_count = 0\n",
    "while(True):\n",
    "    try:\n",
    "        while(data_count<30):       #read data from imu\n",
    "            data0 = ser0.readline().rstrip()\n",
    "            data0 = data0.split()\n",
    "            data1 = ser1.readline().rstrip()\n",
    "            data1 = data1.split()\n",
    "            if (len(data0) == 6 & len(data1) == 6):\n",
    "                imu_data.append([float(time.time()),  int(data0[0]), int(data0[1]), int(data0[2]), int(data0[3]), int(data0[4]), int(data0[5]), int(data1[0]), int(data1[1]), int(data1[2]), int(data1[3]), int(data1[4]), int(data1[5])])\n",
    "                data_count = data_count + 1\n",
    "                \n",
    "        data_count = 0\n",
    "        bucket    = get_buckets(np.array(imu_data[-30:]), 1, 30, 40)   #send the last 30 elements data to get_samples function\n",
    "        predicted = clf.predict(np.reshape(bucket, (1, 40)))     #reshaping and sending for classification\n",
    "        print(bucket)\n",
    "        if(predicted == 1):\n",
    "            pred_count = pred_count + 1\n",
    "            print(\"IMU intent Prediction Count\", pred_count)\n",
    "            if(pred_count>intent_thres_imu):\n",
    "                print(\"Intended Action is XYZ\")\n",
    "            else:\n",
    "                print(\"IMU intent Prediction Count\", pred_count)\n",
    "                pred_count = pred_count - 1\n",
    "    except KeyboardInterrupt:\n",
    "        ser0.flush()\n",
    "        ser0.close()\n",
    "        ser1.flush()\n",
    "        ser1.close()\n",
    "        pass\n",
    "        #keep a counter for it as well\n",
    "        #wait for threshold to predict the intection\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array(imuu)\n",
    "a,b = getPCASample(imuu, inte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#pca = PCA(n_components=1, copy=True, whiten=True)\n",
    "#transformed_dataset = PCA.fit_transform(pca, imuu[0:30])\n",
    "#predicted = clf.predict(transformed_dataset.T)\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "imuu = loadData(\"imu.txt\")\n",
    "inte= loadData(\"intent0.txt\")\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(imuu, inte)\n",
    "#GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "#print(clf.predict([[-0.8, -1]]))\n",
    "\n",
    "#clf_pf = GaussianNB()\n",
    "#clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "#GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "#print(clf_pf.predict([[-0.8, -1]]))\n",
    "\n",
    "predicted = clf.predict(imuu[900:930])\n",
    "print(predicted)\n",
    "#print(transformed_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
